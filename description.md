The dataset available on Kaggle (linked in the work cited section) consists of 3553 color images that belong to 5 categories. These categories are No_DR (no diabetic retinopathy), Mild, Moderate, Severe, and Proliferative. Each image is 256 px x 256 px. I have used a deep neural network model to classify the images in the 5 different categories. There are 370 Mild images, 999 Moderate images, 295 Proliferate_DR images, 193 Severe images, and 1805 No_DR images.

I start training my model using the data I have gathered. I use 20% data for testing and 80% for training. I initially create an image data generator. There are 2 generators used, one for the training images and one for the testing images. I perform data augmentation on the training data by flipping, rotating, resizing, rescaling, and zooming the images. This makes sure that my model generalizes well and does not just memorize the data. I also use a cross validation dataset to make sure that my model does not overfit the training data. I feed images to my model in batches of 32. I divide my data into 3 categories: training, validation, and testing each containing 2490, 439, and 733 image files, respectively. 

I try to mimic the human brain by building this model! The human brain has millions of neurons that communicate with each other. This gives humans the power to see, perceive, smell, think, and do much more things! I similarly use convolutional neural networks and residual network to train my model and it works well with images! I take an image and feed it through layers known as the convolutional layer where I extract important features from the image called feature maps. I then pass these features maps through the pooling layer where the size of feature maps is reduced and compressed. I then flatten the compressed feature maps. This includes taking the pixels and making them in a 1 dimension. This is then fed to the dense connected neural network and the image is then successfully classified and given one of the five labels. 

My model consists of several layers of res-blocks. Res-block consists of 3 layers: convolutional block, and 2 identity blocks.After building the model, I have almost 5 million trainable parameters! I train my model over 10 epochs and then print out a graph to check the model’s loss. The loss keeps reducing after each epoch. The loss of training function is much lower by the end of the 10th epoch as compared to that of the validation data. 

![image](https://github.com/spekhale/diabetic-retinopathy/assets/95264365/5a916e06-65b0-4392-8ef2-3c01c5ef48d6)

The data in the file retina_weights.hdf5 is already trained. I downloaded the file from kaggle and then assessed the model performance. I saw an 85% test accuracy. I also print out the images with their predicted class and original class to give a better idea about the accuracy. I also print out the classification report to see how the model is performing. The model performs really good on the NO_DR class with a precision of 97% and poorly on the Proliferate_DR with a precision of 57%. 

We can improve the model’s accuracy by adding additional training datasets as the dataset that I am using is very unbalanced and hence can affect the accuracy. Performing another data augmentation and trying to improve the generalization capability of the model are some other things to consider for the model to work better.

![image](https://github.com/spekhale/diabetic-retinopathy/assets/95264365/6d0daf4b-02e4-48e5-b955-46af34618b9b)

